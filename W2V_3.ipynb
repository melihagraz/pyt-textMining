{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Description\n",
    "\n",
    "#### Project  <span style=\"color:blue\">NLP for divorce court decision documents</span> \n",
    "\n",
    "#### Content  <span style=\"color:blue\">Word2Vec Clustering</span> \n",
    "\n",
    "#### Notebook Language  <span style=\"color:blue\">python</span> \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/usr/local/lib/python3.5/dist-packages\")\n",
    "import re #for string operations\n",
    "import pandas as pd #for dataframes\n",
    "from gensim.models import Word2Vec #Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kodex_nlp import stem_vocab, diff, read_files #user functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">1</span>  Read .pdf Documents from Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bozma = read_files(\"bozma/\")\n",
    "bozma_onama = read_files(\"bozma-onama/\")\n",
    "all_docs = bozma + bozma_onama\n",
    "joined_docs = ' '.join(all_docs)\n",
    "myFullList = joined_docs.split()\n",
    "myList = list(set(joined_docs.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 26\n",
      "Total number of words in all documents = 3393\n",
      "Total number of unique words in all documents = 800\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents =\",len(all_docs))\n",
    "print(\"Total number of words in all documents =\", len(myFullList))\n",
    "print(\"Total number of unique words in all documents =\", len(myList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">2</span>  Words in Word2Vec Vocablary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('tr/tr.bin')\n",
    "vocab = model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words in W2V vocablary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0% of words are in Vocablary\n"
     ]
    }
   ],
   "source": [
    "print(str(100*np.round(len(list(filter(lambda x: x in vocab, myFullList)))/len(myFullList),2)) + \n",
    "      \"% of words are in Vocablary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words in W2V vocablary after applying stemming to those who are not in vocablary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0% of words are in Vocablary after apllying stemming\n"
     ]
    }
   ],
   "source": [
    "Fwords, FwordsC = stem_vocab(myFullList, vocab)\n",
    "print(str(100*np.round(len(Fwords)/len(myFullList),2)) + \n",
    "      \"% of words are in Vocablary after apllying stemming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">3</span>  Clustering with Word2Vec Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(Fwords))\n",
    "n = len(words)\n",
    "d_M = np.zeros((n, n))\n",
    "\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        d_M[i, j] = model.wv.similarity(words[i], words[j])   \n",
    "        \n",
    "d_M = d_M + d_M.T + np.diag(np.ones(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 12\n"
     ]
    }
   ],
   "source": [
    "db = DBSCAN(eps=1.96, min_samples=5, ).fit(d_M)\n",
    "labels = db.labels_\n",
    "unique_labels = set(labels)\n",
    "n_clusters_ = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(\"Number of Clusters:\", n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 of them labeled as 0\n",
      "===============================\n",
      "14 of them labeled as 1\n",
      "===============================\n",
      "12 of them labeled as 2\n",
      "===============================\n",
      "6 of them labeled as 3\n",
      "===============================\n",
      "24 of them labeled as 4\n",
      "===============================\n",
      "8 of them labeled as 5\n",
      "===============================\n",
      "6 of them labeled as 6\n",
      "===============================\n",
      "7 of them labeled as 7\n",
      "===============================\n",
      "5 of them labeled as 8\n",
      "===============================\n",
      "7 of them labeled as 9\n",
      "===============================\n",
      "5 of them labeled as 10\n",
      "===============================\n",
      "5 of them labeled as 11\n",
      "===============================\n",
      "number of noise = 218\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "for curr_label in unique_labels:\n",
    "      \n",
    "    if curr_label == -1:\n",
    "        print(\"number of noise = \" + str(np.sum(labels == curr_label)))\n",
    "        \n",
    "    else:\n",
    "        print(str(np.sum(labels == curr_label)) + \" of them labeled as \" + str(curr_label))\n",
    "    print(\"===============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(words, label):\n",
    "    return np.array(words)[labels == label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Clusters returned from DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some clusters worked as stemmers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anlaşılmış' 'belirtilmiştir' 'düşünüldü' 'belirtilmiş' 'neden'\n",
      " 'anlaşılmıştır']\n",
      "===========================================================\n",
      "['vermemesi' 'verilmesine' 'alınmadan' 'verilmiş' 'verilip' 'alınmasına'\n",
      " 'verilerek' 'düzenlenmesine']\n",
      "===========================================================\n",
      "['kapsamına' 'yasaya' 'kanuna' 'durumlarına' 'esasına' 'ilkesine']\n",
      "===========================================================\n",
      "['edilmek' 'edildiğine' 'edilemeyecek' 'edilecek' 'edilmeden' 'edileceğini'\n",
      " 'edebileceği']\n"
     ]
    }
   ],
   "source": [
    "print(get_words(words, 3))\n",
    "print(\"===========================================================\")\n",
    "print(get_words(words, 5))\n",
    "print(\"===========================================================\")\n",
    "print(get_words(words, 6))\n",
    "print(\"===========================================================\")\n",
    "print(get_words(words, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### biggest cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ücret' 'hükmü' 'temyiz' 'tazminat' 'yargılama' 'boşanma' 'kanununun'\n",
      " 'bedel' 'davaya' 'mahkeme' 'duruşma' 'kararların' 'usul' 'uyuşmazlık'\n",
      " 'dilekçe' 'uygulanmasını' 'borç' 'mevzuat' 'davalar' 'kanun' 'davaları'\n",
      " 'kararlarını' 'kurul' 'dava']\n"
     ]
    }
   ],
   "source": [
    "print(get_words(words, 4)) # some clusters worked as stemmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noise words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usulü' 'düzenlemesi' 'rahatsızlığı' 'inceleme' 'i' 'açıldığı' 'toplanıp'\n",
      " 'derecesini' 'yürürlükte' 'ayrılık' 'gerekirken' 'yön' 'bahis' 'küfür'\n",
      " 'değer' 'koşullarının' 'dosyası' 'devamında' 'haiz' 'fiil' 'korunma'\n",
      " 'yönlere' 'taktirde' 'dük' 'süresinde' 're' 'dönmesini' 'geçilerek'\n",
      " 'dinlenen' 'gönderilmiş' 'imkân' 'sine' 'be' 'sıfat' 'bozulmuş' 'hastalık'\n",
      " 'birleştirilmiştir' 'yöndeki' 'alışkanlığı' 'savunmanın' 'görgü'\n",
      " 'bölümlerin' 'boşluk' 'makam' 'birliğinin' 'taraf' 'semt' 'hüküm' 'çoktur'\n",
      " 'oluşturulmuş' 'düzenlemeleri' 'koyma' 'birliğin' 'ayrılmak' 'verilmek'\n",
      " 'zina' 'oluşmuştur' 'gününün' 'avukatlık' 'geçirilerek' 'bulunmamaktadır'\n",
      " 'karar' 'eldeki' 'kağıt' 'sağlayıcı' 'gerçekleştiği' 'şartlarını'\n",
      " 'anlatım' 'mümkünse' 'niteliğindedir' 'azdır' 'yapılmadığı' 'ya' 'belge'\n",
      " 'doğurur' 'ilkeler' 'nazaran' 'şartlarının' 'yoruma' 'gerek' 'usulüne'\n",
      " 'aşaması' 'kuşkusuz' 'görülmemiş' 'çözüm' 'vekil' 'alacak' 'müşterek'\n",
      " 'bozma' 'bölümlerinin' 'yasemin' 'birleştirilerek' 'oluşturmak'\n",
      " 'yapıldıktan' 'tehlikesiyle' 'olay' 'tamamına' 'atıf' 'ir' 'irtibat'\n",
      " 'geçirir' 'toplanarak' 'eşiyle' 'fazla' 'sabittir' 'belgelerin' 'başka'\n",
      " 'agresif' 'daire' 'ettiren' 'sebebiyet' 'gebelik' 'dayandığı' 'ağırlık'\n",
      " 'dosya' 'diğerini' 'olunması' 'aşamasına' 'ila' 'inceler' 'birleşen'\n",
      " 'gider' 'sebep' 'sonuç' 'sınırlama' 'kaynaklandığı' 'dereceleri'\n",
      " 'işleminin' 'evrak' 'yaşamak' 'sebeplerden' 'nitelik' 'yüklenmesi'\n",
      " 'serbestçe' 'erkek' 'olun' 'sonra' 'kurulunca' 'zorunludur' 'ne'\n",
      " 'uygulayan' 'öküz' 'yazı' 'ulaşılması' 'yüklenen' 'işleriyle' 'dosyalar'\n",
      " 'gönderilmesi' 'dayanılarak' 'niteliğindeki' 'nedenden' 'dayanak'\n",
      " 'yapılma' 'amaç' 'eşyalarını' 'eş' 'mahkemesinde' 'du' 'bütünlük'\n",
      " 'belirler' 'yaratılmıştır' 'kahvehane' 'incelemede' 'alışkanlık'\n",
      " 'değerlerinin' 'miktarını' 'eklenerek' 'yapılmamış' 'bozulması' 'rifat'\n",
      " 'düzeltme' 'yaratılmış' 'kat' 'reddi' 'belirlenir' 'gösterilmeye'\n",
      " 'anlaşma' 'yaşamakta' 'kıvanç' 'konularını' 'bildirmiştir' 'sunma'\n",
      " 'telefon' 'eki' 'deli' 'derece' 'olan' 'eşya' 'tanık' 'ayrıldığı' 'kabulü'\n",
      " 'bırakılması' 'irade' 'ona' 'çekişme' 'koşullarına' 'zorlayan' 'temel'\n",
      " 'dayanağı' 'belirleme' 'etkileyecek' 'sebeplerle' 'birleştirme'\n",
      " 'öngördüğü' 'özet' 'dinlenme' 'sas' 'yaratacak' 'uzak' 'yapılıp'\n",
      " 'birliğine' 'miktar' 'yan' 'eşleri' 'bozulan' 'zeka' 'bozulmasına'\n",
      " 'savunma']\n"
     ]
    }
   ],
   "source": [
    "print(get_words(words, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
